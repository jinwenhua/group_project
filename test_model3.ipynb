{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/save/cb_model/movie-corpus/2-2_500/4000_checkpoint.tar\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "embedded.size <built-in method size of Tensor object at 0x139fc6d40>\n",
      "hidden.shape torch.Size([4, 1, 500])\n",
      "outputs.shape torch.Size([10, 1, 500])\n",
      "embedded.size <built-in method size of Tensor object at 0x139fc4ef0>\n",
      "hidden.shape torch.Size([4, 1, 500])\n",
      "outputs.shape torch.Size([10, 1, 500])\n",
      "embedded.size <built-in method size of Tensor object at 0x139fc6c50>\n",
      "hidden.shape torch.Size([4, 1, 500])\n",
      "outputs.shape torch.Size([10, 1, 500])\n",
      "> hello\n",
      "[2, 1]\n",
      "2\n",
      "(1,.,.) = \n",
      " Columns 1 to 9  0.3369  0.2775  0.1789  0.3354  0.4945 -0.2123  0.3613 -0.2252  0.7009\n",
      "\n",
      "Columns 10 to 18  0.1049  0.4266 -0.2466  0.0073 -0.6312 -0.2285 -0.1628 -0.0642 -0.0012\n",
      "\n",
      "Columns 19 to 27  0.7369 -0.0205 -0.0248 -0.1497 -0.3176 -0.1479  0.3756  0.1009 -0.0902\n",
      "\n",
      "Columns 28 to 36  0.5370 -0.3987  0.1504  0.0373 -0.2532  0.3200 -0.0916 -0.3074 -0.0092\n",
      "\n",
      "Columns 37 to 45 -0.5106 -0.9228 -0.6873 -0.1341 -0.2168  0.0276 -0.1705 -0.1497  0.0027\n",
      "\n",
      "Columns 46 to 54  0.1136 -0.1660  0.1076 -0.1506  0.1742  0.1033 -0.0647 -0.4698  0.1477\n",
      "\n",
      "Columns 55 to 63  0.1067  0.1273 -0.7360  0.1717  0.3389  0.3553  0.1400 -0.1096  0.3516\n",
      "\n",
      "Columns 64 to 72 -0.2889 -0.0975  0.2776  0.0305  0.3505 -0.1660  0.0836 -0.0272  0.1947\n",
      "\n",
      "Columns 73 to 81 -0.2656 -0.5600 -0.4770  0.0426  0.3530 -0.5001  0.2198  0.0199  0.0577\n",
      "\n",
      "Columns 82 to 90 -0.2187 -0.0308 -0.5707  0.3549 -0.0110 -0.6897 -0.3024 -0.0329  0.1944\n",
      "\n",
      "Columns 91 to 99 -0.4439  0.2659 -0.3600  0.1121 -0.4110 -0.2070 -0.8924  0.2885 -0.3969\n",
      "\n",
      "Columns 100 to 108  0.0724 -0.2909  0.0475  0.1406  0.6792  0.0968 -0.4759 -0.5548  0.2104\n",
      "\n",
      "Columns 109 to 117  0.0392 -0.4477  0.1543  0.2012  0.2093 -0.2595 -0.1140  0.4172 -0.7194\n",
      "\n",
      "Columns 118 to 126  0.7505 -0.4379 -0.2469 -0.0405 -0.6560 -0.1462 -0.3652  0.2541  0.3743\n",
      "\n",
      "Columns 127 to 135  0.3333 -0.1343  0.3233 -0.2455  0.1811 -0.3840 -0.1751  0.3571  0.4577\n",
      "\n",
      "Columns 136 to 144 -0.1366  0.1182 -0.4079  0.0656 -0.3017  0.2655  0.3876 -0.2958 -0.1291\n",
      "\n",
      "Columns 145 to 153  0.9483  0.2566  0.2873  0.2450 -0.1709  0.0548  0.1637 -0.4755 -0.0763\n",
      "\n",
      "Columns 154 to 162  0.0165 -0.3345 -0.1012  0.1271 -0.3882  0.0503  0.3700  0.2569  0.6003\n",
      "\n",
      "Columns 163 to 171  0.5187 -0.2883  0.0267  0.3547  0.2523  0.2756  0.2991 -0.5507 -0.1087\n",
      "\n",
      "Columns 172 to 180 -0.0107 -0.0014 -0.8545  0.2676  0.1610  0.4333 -0.4097  0.3005  0.4513\n",
      "\n",
      "Columns 181 to 189  0.4844  0.6946 -0.4514 -0.1715 -0.1670  0.3063  0.7808  0.5650 -0.1995\n",
      "\n",
      "Columns 190 to 198  0.2256  0.3278 -0.0497  0.6093  0.1259 -0.2244 -0.1492  0.1573  0.1269\n",
      "\n",
      "Columns 199 to 207  0.3248 -0.2598 -0.6650  0.0204  0.0598 -0.1408 -0.0780  0.4437 -0.0031\n",
      "\n",
      "Columns 208 to 216 -0.6504  0.3349  0.2570 -0.0044 -0.4886  0.0661  0.0792  0.0569  0.2050\n",
      "\n",
      "Columns 217 to 225 -0.6353  0.3599  0.0804  0.3423  0.0270  0.1911  0.0661  0.0748  0.3170\n",
      "\n",
      "Columns 226 to 234 -0.5100 -0.0286 -0.5128 -0.3789  0.3753  0.2738  0.2871  0.1658 -0.3331\n",
      "\n",
      "Columns 235 to 243 -0.0627  0.3098  0.1712  0.2324  0.0836 -0.6649 -0.2784  0.3730  0.4300\n",
      "\n",
      "Columns 244 to 252  0.5252 -0.0017  0.0117 -0.1306 -0.3024  0.0139 -0.4705  0.4861  0.0218\n",
      "\n",
      "Columns 253 to 261 -0.2527  0.5391 -0.4116  0.4235 -0.0932  0.1734  0.4204 -0.2714  0.5590\n",
      "\n",
      "Columns 262 to 270  0.3639  0.1320  0.0860 -0.5825 -0.0916  0.1750  0.6094  0.7665 -0.3325\n",
      "\n",
      "Columns 271 to 279 -0.3664  0.4706  0.2132 -0.1656  0.0956 -0.0081  0.0768 -0.4239 -0.1230\n",
      "\n",
      "Columns 280 to 288 -0.2103  0.0800 -0.0916 -0.3155 -0.6327  0.5176 -0.0589  0.2608  0.4134\n",
      "\n",
      "Columns 289 to 297 -0.3079 -0.5918  0.6940  0.0219 -0.5712 -0.5283  0.1866 -0.3874 -0.2751\n",
      "\n",
      "Columns 298 to 306 -0.1235  0.0586 -0.2442 -0.2898  0.1179 -0.1545 -0.0862 -0.3145 -0.0167\n",
      "\n",
      "Columns 307 to 315  0.0868  0.0509 -0.1658 -0.4003  0.1690  0.0636  0.0436 -0.5262  0.0423\n",
      "\n",
      "Columns 316 to 324  0.0682  0.0765  0.1399  0.0929 -0.2576  0.0645  0.8972  0.1001 -0.2646\n",
      "\n",
      "Columns 325 to 333  0.2685  0.0962  0.7409 -0.1751 -0.4019 -0.4899  0.4077 -0.0263 -0.7182\n",
      "\n",
      "Columns 334 to 342  0.3943  0.0366  0.5932 -0.7202  0.2275 -0.2765  0.1070  0.8899  0.1562\n",
      "\n",
      "Columns 343 to 351 -0.4411  0.2843 -0.1672 -0.8517 -0.1796 -0.5207 -0.1945  0.4013  0.2172\n",
      "\n",
      "Columns 352 to 360 -0.0543 -0.2325 -0.1145 -0.3019  0.1829  0.1004  0.4469  0.6933  0.1332\n",
      "\n",
      "Columns 361 to 369  0.5664 -0.1227  0.2847  0.1608  0.4892 -0.8555 -0.5736  0.2213 -0.3061\n",
      "\n",
      "Columns 370 to 378 -0.5069 -0.0313  0.1469 -0.3152 -0.3821  0.1160  0.7050  0.0398 -0.1440\n",
      "\n",
      "Columns 379 to 387  0.2495  0.1837 -0.0969 -0.1961  0.1283  0.3664 -0.3433 -0.5057  0.0056\n",
      "\n",
      "Columns 388 to 396 -0.2033  0.2056 -0.3397 -0.3683  0.2276 -0.5880 -0.2890 -0.3877 -0.0671\n",
      "\n",
      "Columns 397 to 405 -0.3758 -0.3911 -0.2748  0.6148  0.0426 -0.2327 -0.3770  0.5096 -0.0587\n",
      "\n",
      "Columns 406 to 414  0.4710  0.0988  0.1092  0.1890 -0.0883 -0.0564 -0.0984 -0.2927 -0.4276\n",
      "\n",
      "Columns 415 to 423 -0.1100  0.0425  0.3407  0.3497 -0.1370  0.2555 -0.1151  0.0780 -0.1029\n",
      "\n",
      "Columns 424 to 432 -0.0475 -0.2496 -0.1438  0.0725  0.0714 -0.3336  0.2926  0.0516  0.0633\n",
      "\n",
      "Columns 433 to 441  0.1158  0.0270 -0.1739  0.1406  0.0370  0.2867 -0.1153  0.1279 -0.0934\n",
      "\n",
      "Columns 442 to 450 -0.0435 -0.2886  0.2820  0.0625  0.2064 -0.7862 -0.0240 -0.2361 -0.0953\n",
      "\n",
      "Columns 451 to 459  0.1011  0.4144 -0.5754  0.0955 -0.1142 -0.0473  0.0068  0.2352  0.6583\n",
      "\n",
      "Columns 460 to 468 -0.3067 -0.2276 -0.4945  0.6626 -0.0847 -0.1856  0.0884  0.1642  0.1551\n",
      "\n",
      "Columns 469 to 477 -0.0411  0.8773  0.1493 -0.1508 -0.0349  0.1056 -0.2210 -0.0396  0.2747\n",
      "\n",
      "Columns 478 to 486  0.7492  0.1519  0.3109  0.0435 -0.0321  0.3225 -0.3008  0.5334  0.3890\n",
      "\n",
      "Columns 487 to 495  0.1597  0.1911  0.3064 -0.4703 -0.6482 -0.5333 -0.6534 -0.0433 -0.4066\n",
      "\n",
      "Columns 496 to 500  0.3174  0.1223  0.6084 -0.5554 -0.3223\n",
      "\n",
      "(2,.,.) = \n",
      " Columns 1 to 9  0.5490  0.1146  0.2736  0.3259  0.2196 -0.0920  0.4426 -0.0429  0.5588\n",
      "\n",
      "Columns 10 to 18  0.1902  0.3827 -0.0842 -0.0389 -0.3230 -0.3104 -0.1613 -0.0410 -0.0358\n",
      "\n",
      "Columns 19 to 27  0.6691 -0.4884 -0.0702 -0.2729 -0.2375 -0.1995  0.3998 -0.0931 -0.1656\n",
      "\n",
      "Columns 28 to 36  0.4269 -0.2502  0.4017  0.1729 -0.1170  0.3051  0.1467 -0.2248  0.2300\n",
      "\n",
      "Columns 37 to 45 -0.5390 -0.9376 -0.4951 -0.2151 -0.2907 -0.0673 -0.3194 -0.1209 -0.1426\n",
      "\n",
      "Columns 46 to 54 -0.4575 -0.0953 -0.2225 -0.0504  0.3330 -0.1264 -0.0296 -0.5329  0.0867\n",
      "\n",
      "Columns 55 to 63  0.2359 -0.2769 -0.5247 -0.0656 -0.0146  0.2631  0.3811 -0.0426  0.3943\n",
      "\n",
      "Columns 64 to 72 -0.0785 -0.0594  0.2401  0.0692  0.0872  0.1189  0.2743 -0.1131  0.1421\n",
      "\n",
      "Columns 73 to 81 -0.2657 -0.7337 -0.3726 -0.0754  0.0867 -0.0992  0.2431  0.0117  0.0902\n",
      "\n",
      "Columns 82 to 90 -0.3080 -0.0014 -0.5385  0.1900 -0.0066 -0.5208 -0.2514  0.0623  0.4315\n",
      "\n",
      "Columns 91 to 99 -0.3804  0.1605 -0.2616 -0.0636 -0.0301 -0.1494 -0.7352  0.4053 -0.4243\n",
      "\n",
      "Columns 100 to 108  0.3029  0.1177 -0.0596  0.0646  0.4212 -0.1161 -0.3763 -0.5508  0.0076\n",
      "\n",
      "Columns 109 to 117  0.1829 -0.1709 -0.1830 -0.0707 -0.1817 -0.2055 -0.3348  0.3089 -0.1373\n",
      "\n",
      "Columns 118 to 126  0.7809 -0.3850 -0.0037  0.2158 -0.4924  0.0798 -0.3432  0.2570  0.2055\n",
      "\n",
      "Columns 127 to 135  0.5515  0.0379  0.2609  0.0861  0.0220 -0.1681  0.0237  0.2900  0.3037\n",
      "\n",
      "Columns 136 to 144  0.0443  0.4126 -0.2262  0.0235 -0.2116  0.1856  0.3254 -0.1211  0.0354\n",
      "\n",
      "Columns 145 to 153  0.7101  0.0572  0.1360  0.1125 -0.2215 -0.2904  0.5159 -0.4507 -0.3187\n",
      "\n",
      "Columns 154 to 162 -0.0411 -0.1785  0.1227  0.2990 -0.3754 -0.1672  0.1718  0.3142  0.6764\n",
      "\n",
      "Columns 163 to 171  0.4452 -0.1574  0.1315  0.3552  0.3623  0.0334  0.0056 -0.2339  0.0151\n",
      "\n",
      "Columns 172 to 180  0.4017  0.2174 -0.7417  0.5161  0.3910  0.0143 -0.3237  0.3404  0.5948\n",
      "\n",
      "Columns 181 to 189  0.2676  0.7830 -0.3827  0.0448 -0.1958  0.1451  0.8091  0.4558 -0.0785\n",
      "\n",
      "Columns 190 to 198  0.2228  0.2909 -0.1246  0.4079 -0.0835 -0.3422 -0.2123  0.1952  0.1487\n",
      "\n",
      "Columns 199 to 207  0.3941 -0.4306 -0.5802 -0.0122  0.0433 -0.6519  0.0657  0.3914  0.0603\n",
      "\n",
      "Columns 208 to 216 -0.4526  0.0175  0.3562  0.1288 -0.2340  0.2242  0.0864 -0.0779  0.0568\n",
      "\n",
      "Columns 217 to 225 -0.4157  0.4925  0.0535  0.4550  0.1523  0.2612  0.1277 -0.1400  0.0584\n",
      "\n",
      "Columns 226 to 234 -0.6479  0.1411 -0.5154 -0.2260  0.3844  0.2483  0.1444  0.5119 -0.1598\n",
      "\n",
      "Columns 235 to 243 -0.0117  0.5546 -0.0331  0.3543  0.1699 -0.5415 -0.4027  0.2174  0.3573\n",
      "\n",
      "Columns 244 to 252  0.4735 -0.0078 -0.2348 -0.0319 -0.2087 -0.0063 -0.5754  0.2294 -0.0202\n",
      "\n",
      "Columns 253 to 261 -0.2467  0.1517 -0.4788  0.4115 -0.0406  0.2181  0.5940 -0.2260  0.3489\n",
      "\n",
      "Columns 262 to 270  0.1822  0.2461 -0.0644 -0.5894 -0.0049  0.1623  0.4154  0.5038 -0.0761\n",
      "\n",
      "Columns 271 to 279 -0.2843  0.4138 -0.0724 -0.1297 -0.0206 -0.0713 -0.0393 -0.5414  0.1193\n",
      "\n",
      "Columns 280 to 288 -0.3929  0.3624 -0.2915 -0.5644 -0.4808  0.4216  0.2244  0.1069  0.0637\n",
      "\n",
      "Columns 289 to 297 -0.2779 -0.2512  0.7321  0.1822 -0.4817 -0.4718  0.1900 -0.4181 -0.0248\n",
      "\n",
      "Columns 298 to 306  0.0869 -0.1346 -0.3642 -0.4779  0.2782  0.0446 -0.1183 -0.3529 -0.6120\n",
      "\n",
      "Columns 307 to 315 -0.1810  0.0698 -0.1011 -0.5870  0.0657 -0.0895 -0.0262 -0.5372  0.0375\n",
      "\n",
      "Columns 316 to 324 -0.1149  0.0159 -0.0392  0.0992 -0.1606  0.1001  0.7711  0.0338 -0.2945\n",
      "\n",
      "Columns 325 to 333  0.1552 -0.1464  0.6066  0.0569 -0.4233 -0.4146  0.2163 -0.0296 -0.4998\n",
      "\n",
      "Columns 334 to 342  0.4402 -0.0711  0.4679 -0.6161  0.2697 -0.0337  0.2199  0.5027  0.2353\n",
      "\n",
      "Columns 343 to 351 -0.6377 -0.0624 -0.1314 -0.7939 -0.0512 -0.3799 -0.1923  0.0698  0.3349\n",
      "\n",
      "Columns 352 to 360  0.0840 -0.0485 -0.2066 -0.0343 -0.0785  0.0585  0.6206  0.5060  0.0085\n",
      "\n",
      "Columns 361 to 369  0.4165  0.0849  0.2987  0.3572  0.2670 -0.6994 -0.4991 -0.1115 -0.2741\n",
      "\n",
      "Columns 370 to 378 -0.6551 -0.0485  0.1152 -0.4777 -0.3843  0.4442  0.6024  0.0175 -0.1114\n",
      "\n",
      "Columns 379 to 387 -0.1308  0.0915 -0.0554 -0.0491  0.0699  0.2319 -0.3079 -0.4259 -0.1754\n",
      "\n",
      "Columns 388 to 396 -0.1727  0.0576  0.0106 -0.3393  0.0010 -0.3530 -0.1347 -0.4560 -0.0662\n",
      "\n",
      "Columns 397 to 405 -0.3996 -0.2145 -0.0188  0.6325  0.0881 -0.0075 -0.3122  0.3488 -0.1323\n",
      "\n",
      "Columns 406 to 414  0.3955  0.1063  0.0032  0.3578 -0.0588 -0.0871  0.1289  0.1701 -0.3613\n",
      "\n",
      "Columns 415 to 423 -0.3725  0.1336  0.3547  0.2941 -0.0976 -0.0596 -0.1873 -0.0316 -0.1833\n",
      "\n",
      "Columns 424 to 432  0.1163 -0.1512 -0.0842  0.0233 -0.0146  0.0601  0.4886 -0.2035 -0.0065\n",
      "\n",
      "Columns 433 to 441  0.0013 -0.0388 -0.4082  0.4250  0.0609  0.2757  0.0180 -0.1167  0.1307\n",
      "\n",
      "Columns 442 to 450  0.0721 -0.1952  0.0432  0.0197  0.2576 -0.4423  0.1426 -0.0296 -0.1403\n",
      "\n",
      "Columns 451 to 459 -0.1959  0.3506 -0.4822  0.3483  0.0547  0.0819  0.0716  0.1026  0.7400\n",
      "\n",
      "Columns 460 to 468 -0.1184 -0.4160 -0.3179  0.3011  0.0539 -0.2003 -0.0439 -0.0087  0.2857\n",
      "\n",
      "Columns 469 to 477  0.2689  0.8986 -0.0418 -0.1576  0.0218 -0.1132 -0.2290 -0.1958  0.5707\n",
      "\n",
      "Columns 478 to 486  0.4813  0.1218  0.1196 -0.0365  0.2802  0.2670 -0.1726  0.1358  0.1585\n",
      "\n",
      "Columns 487 to 495  0.0107  0.4112  0.2720 -0.3013 -0.4476 -0.3668 -0.6702 -0.1037 -0.0135\n",
      "\n",
      "Columns 496 to 500  0.0306  0.0246  0.6381 -0.4609 -0.1181\n",
      "[ CPUFloatType{2,1,500} ]\n",
      "Bot: hello . . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/jit/_trace.py:154: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:485.)\n",
      "  if a.grad is not None:\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Deploying a Seq2Seq Model with TorchScript\n",
    "==================================================\n",
    "**Author:** `Matthew Inkawhich <https://github.com/MatthewInkawhich>`_\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# This tutorial will walk through the process of transitioning a\n",
    "# sequence-to-sequence model to TorchScript using the TorchScript\n",
    "# API. The model that we will convert is the chatbot model from the\n",
    "# `Chatbot tutorial <https://pytorch.org/tutorials/beginner/chatbot_tutorial.html>`__.\n",
    "# You can either treat this tutorial as a “Part 2” to the Chatbot tutorial\n",
    "# and deploy your own pretrained model, or you can start with this\n",
    "# document and use a pretrained model that we host. In the latter case,\n",
    "# you can reference the original Chatbot tutorial for details\n",
    "# regarding data preprocessing, model theory and definition, and model\n",
    "# training.\n",
    "#\n",
    "# What is TorchScript?\n",
    "# ----------------------------\n",
    "#\n",
    "# During the research and development phase of a deep learning-based\n",
    "# project, it is advantageous to interact with an **eager**, imperative\n",
    "# interface like PyTorch’s. This gives users the ability to write\n",
    "# familiar, idiomatic Python, allowing for the use of Python data\n",
    "# structures, control flow operations, print statements, and debugging\n",
    "# utilities. Although the eager interface is a beneficial tool for\n",
    "# research and experimentation applications, when it comes time to deploy\n",
    "# the model in a production environment, having a **graph**-based model\n",
    "# representation is very beneficial. A deferred graph representation\n",
    "# allows for optimizations such as out-of-order execution, and the ability\n",
    "# to target highly optimized hardware architectures. Also, a graph-based\n",
    "# representation enables framework-agnostic model exportation. PyTorch\n",
    "# provides mechanisms for incrementally converting eager-mode code into\n",
    "# TorchScript, a statically analyzable and optimizable subset of Python\n",
    "# that Torch uses to represent deep learning programs independently from\n",
    "# the Python runtime.\n",
    "#\n",
    "# The API for converting eager-mode PyTorch programs into TorchScript is\n",
    "# found in the torch.jit module. This module has two core modalities for\n",
    "# converting an eager-mode model to a TorchScript graph representation:\n",
    "# **tracing** and **scripting**. The ``torch.jit.trace`` function takes a\n",
    "# module or function and a set of example inputs. It then runs the example\n",
    "# input through the function or module while tracing the computational\n",
    "# steps that are encountered, and outputs a graph-based function that\n",
    "# performs the traced operations. **Tracing** is great for straightforward\n",
    "# modules and functions that do not involve data-dependent control flow,\n",
    "# such as standard convolutional neural networks. However, if a function\n",
    "# with data-dependent if statements and loops is traced, only the\n",
    "# operations called along the execution route taken by the example input\n",
    "# will be recorded. In other words, the control flow itself is not\n",
    "# captured. To convert modules and functions containing data-dependent\n",
    "# control flow, a **scripting** mechanism is provided. The\n",
    "# ``torch.jit.script`` function/decorator takes a module or function and\n",
    "# does not requires example inputs. Scripting then explicitly converts\n",
    "# the module or function code to TorchScript, including all control flows.\n",
    "# One caveat with using scripting is that it only supports a subset of\n",
    "# Python, so you might need to rewrite the code to make it compatible\n",
    "# with the TorchScript syntax.\n",
    "#\n",
    "# For all details relating to the supported features, see the `TorchScript\n",
    "# language reference <https://pytorch.org/docs/master/jit.html>`__.\n",
    "# To provide the maximum flexibility, you can also mix tracing and scripting\n",
    "# modes together to represent your whole program, and these techniques can\n",
    "# be applied incrementally.\n",
    "#\n",
    "# .. figure:: /_static/img/chatbot/pytorch_workflow.png\n",
    "#    :align: center\n",
    "#    :alt: workflow\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Acknowledgements\n",
    "# ----------------\n",
    "#\n",
    "# This tutorial was inspired by the following sources:\n",
    "#\n",
    "# 1) Yuan-Kuei Wu’s pytorch-chatbot implementation:\n",
    "#    https://github.com/ywk991112/pytorch-chatbot\n",
    "#\n",
    "# 2) Sean Robertson’s practical-pytorch seq2seq-translation example:\n",
    "#    https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation\n",
    "#\n",
    "# 3) FloydHub’s Cornell Movie Corpus preprocessing code:\n",
    "#    https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus\n",
    "#\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Prepare Environment\n",
    "# -------------------\n",
    "#\n",
    "# First, we will import the required modules and set some constants. If\n",
    "# you are planning on using your own model, be sure that the\n",
    "# ``MAX_LENGTH`` constant is set correctly. As a reminder, this constant\n",
    "# defines the maximum allowed sentence length during training and the\n",
    "# maximum length output that the model is capable of producing.\n",
    "#\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "from model import EncoderRNN, Attn, LuongAttnDecoderRNN\n",
    "from Voc import *\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, decoder_n_layers):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self._device = device\n",
    "        self._SOS_token = SOS_token\n",
    "        self._decoder_n_layers = decoder_n_layers\n",
    "\n",
    "    __constants__ = ['_device', '_SOS_token', '_decoder_n_layers']\n",
    "\n",
    "    def forward(self, input_seq : torch.Tensor, input_length : torch.Tensor, max_length : int):\n",
    "        # Forward input through encoder model\n",
    "        print(input_seq.shape)\n",
    "        print(len(input_seq.shape))\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        print(encoder_outputs)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:self._decoder_n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=self._device, dtype=torch.long) * self._SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=self._device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=self._device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores\n",
    "def evaluate(searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    #print(tokens)\n",
    "    #print(scores)\n",
    "    # indexes -> words\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    #print(decoded_words)\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "# Evaluate inputs from user input (stdin)\n",
    "def evaluateInput(searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            # Evaluate sentence\n",
    "            output_words = evaluate(searcher, voc, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")\n",
    "\n",
    "# Normalize input sentence and call evaluate()\n",
    "def evaluateExample(sentence, searcher, voc):\n",
    "    print(\"> \" + sentence)\n",
    "    # Normalize sentence\n",
    "    input_sentence = normalizeString(sentence)\n",
    "    #print(input_sentence)\n",
    "    # Evaluate sentence\n",
    "    output_words = evaluate(searcher, voc, input_sentence)\n",
    "    output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "    print('Bot:', ' '.join(output_words))\n",
    "\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "corpus_name = \"movie-corpus\"\n",
    "\n",
    "# Configure models\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# If you're loading your own model\n",
    "# Set checkpoint to load from\n",
    "checkpoint_iter = 4000\n",
    "loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "                             '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "                             '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "print(loadFilename)\n",
    "\n",
    "# If you're loading the hosted model\n",
    "#loadFilename = 'data/4000_checkpoint.tar'\n",
    "\n",
    "# Load model\n",
    "# Force CPU device options (to match tensors in this tutorial)\n",
    "checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "encoder_sd = checkpoint['en']\n",
    "decoder_sd = checkpoint['de']\n",
    "encoder_optimizer_sd = checkpoint['en_opt']\n",
    "decoder_optimizer_sd = checkpoint['de_opt']\n",
    "embedding_sd = checkpoint['embedding']\n",
    "voc = Voc(corpus_name)\n",
    "voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "# Load trained model params\n",
    "encoder.load_state_dict(encoder_sd)\n",
    "decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "print('Models built and ready to go!')\n",
    "\n",
    "\n",
    "### Compile the whole greedy search model to TorchScript model\n",
    "# Create artificial inputs\n",
    "test_seq = torch.LongTensor(MAX_LENGTH, 1).random_(0, voc.num_words).to(device)\n",
    "test_seq_length = torch.LongTensor([test_seq.size()[0]]).to(device)\n",
    "# Trace the model\n",
    "traced_encoder = torch.jit.trace(encoder, (test_seq, test_seq_length))\n",
    "\n",
    "### Convert decoder model\n",
    "# Create and generate artificial inputs\n",
    "test_encoder_outputs, test_encoder_hidden = traced_encoder(test_seq, test_seq_length)\n",
    "test_decoder_hidden = test_encoder_hidden[:decoder.n_layers]\n",
    "test_decoder_input = torch.LongTensor(1, 1).random_(0, voc.num_words)\n",
    "# Trace the model\n",
    "traced_decoder = torch.jit.trace(decoder, (test_decoder_input, test_decoder_hidden, test_encoder_outputs))\n",
    "\n",
    "### Initialize searcher module by wrapping ``torch.jit.script`` call\n",
    "scripted_searcher = torch.jit.script(GreedySearchDecoder(traced_encoder, traced_decoder, decoder.n_layers))\n",
    "\n",
    "\n",
    "# Use appropriate device\n",
    "scripted_searcher.to(device)\n",
    "# Set dropout layers to eval mode\n",
    "scripted_searcher.eval()\n",
    "\n",
    "# Evaluate examples\n",
    "#sentences = [\"hello\", \"what's up?\", \"who are you?\", \"where am I?\", \"where are you from?\"]\n",
    "sentences = [\"hello\"]\n",
    "for s in sentences:\n",
    "    evaluateExample(s, scripted_searcher, voc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

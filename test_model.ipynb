{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Deploying a Seq2Seq Model with TorchScript\n",
    "==================================================\n",
    "**Author:** `Matthew Inkawhich <https://github.com/MatthewInkawhich>`_\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# This tutorial will walk through the process of transitioning a\n",
    "# sequence-to-sequence model to TorchScript using the TorchScript\n",
    "# API. The model that we will convert is the chatbot model from the\n",
    "# `Chatbot tutorial <https://pytorch.org/tutorials/beginner/chatbot_tutorial.html>`__.\n",
    "# You can either treat this tutorial as a “Part 2” to the Chatbot tutorial\n",
    "# and deploy your own pretrained model, or you can start with this\n",
    "# document and use a pretrained model that we host. In the latter case,\n",
    "# you can reference the original Chatbot tutorial for details\n",
    "# regarding data preprocessing, model theory and definition, and model\n",
    "# training.\n",
    "#\n",
    "# What is TorchScript?\n",
    "# ----------------------------\n",
    "#\n",
    "# During the research and development phase of a deep learning-based\n",
    "# project, it is advantageous to interact with an **eager**, imperative\n",
    "# interface like PyTorch’s. This gives users the ability to write\n",
    "# familiar, idiomatic Python, allowing for the use of Python data\n",
    "# structures, control flow operations, print statements, and debugging\n",
    "# utilities. Although the eager interface is a beneficial tool for\n",
    "# research and experimentation applications, when it comes time to deploy\n",
    "# the model in a production environment, having a **graph**-based model\n",
    "# representation is very beneficial. A deferred graph representation\n",
    "# allows for optimizations such as out-of-order execution, and the ability\n",
    "# to target highly optimized hardware architectures. Also, a graph-based\n",
    "# representation enables framework-agnostic model exportation. PyTorch\n",
    "# provides mechanisms for incrementally converting eager-mode code into\n",
    "# TorchScript, a statically analyzable and optimizable subset of Python\n",
    "# that Torch uses to represent deep learning programs independently from\n",
    "# the Python runtime.\n",
    "#\n",
    "# The API for converting eager-mode PyTorch programs into TorchScript is\n",
    "# found in the torch.jit module. This module has two core modalities for\n",
    "# converting an eager-mode model to a TorchScript graph representation:\n",
    "# **tracing** and **scripting**. The ``torch.jit.trace`` function takes a\n",
    "# module or function and a set of example inputs. It then runs the example\n",
    "# input through the function or module while tracing the computational\n",
    "# steps that are encountered, and outputs a graph-based function that\n",
    "# performs the traced operations. **Tracing** is great for straightforward\n",
    "# modules and functions that do not involve data-dependent control flow,\n",
    "# such as standard convolutional neural networks. However, if a function\n",
    "# with data-dependent if statements and loops is traced, only the\n",
    "# operations called along the execution route taken by the example input\n",
    "# will be recorded. In other words, the control flow itself is not\n",
    "# captured. To convert modules and functions containing data-dependent\n",
    "# control flow, a **scripting** mechanism is provided. The\n",
    "# ``torch.jit.script`` function/decorator takes a module or function and\n",
    "# does not requires example inputs. Scripting then explicitly converts\n",
    "# the module or function code to TorchScript, including all control flows.\n",
    "# One caveat with using scripting is that it only supports a subset of\n",
    "# Python, so you might need to rewrite the code to make it compatible\n",
    "# with the TorchScript syntax.\n",
    "#\n",
    "# For all details relating to the supported features, see the `TorchScript\n",
    "# language reference <https://pytorch.org/docs/master/jit.html>`__.\n",
    "# To provide the maximum flexibility, you can also mix tracing and scripting\n",
    "# modes together to represent your whole program, and these techniques can\n",
    "# be applied incrementally.\n",
    "#\n",
    "# .. figure:: /_static/img/chatbot/pytorch_workflow.png\n",
    "#    :align: center\n",
    "#    :alt: workflow\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Acknowledgements\n",
    "# ----------------\n",
    "#\n",
    "# This tutorial was inspired by the following sources:\n",
    "#\n",
    "# 1) Yuan-Kuei Wu’s pytorch-chatbot implementation:\n",
    "#    https://github.com/ywk991112/pytorch-chatbot\n",
    "#\n",
    "# 2) Sean Robertson’s practical-pytorch seq2seq-translation example:\n",
    "#    https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation\n",
    "#\n",
    "# 3) FloydHub’s Cornell Movie Corpus preprocessing code:\n",
    "#    https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus\n",
    "#\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Prepare Environment\n",
    "# -------------------\n",
    "#\n",
    "# First, we will import the required modules and set some constants. If\n",
    "# you are planning on using your own model, be sure that the\n",
    "# ``MAX_LENGTH`` constant is set correctly. As a reminder, this constant\n",
    "# defines the maximum allowed sentence length during training and the\n",
    "# maximum length output that the model is capable of producing.\n",
    "#\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "from model import EncoderRNN, Attn, LuongAttnDecoderRNN\n",
    "from Voc import *\n",
    "\n",
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Define Evaluation\n",
    "# -----------------\n",
    "#\n",
    "# Greedy Search Decoder\n",
    "# ~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# As in the chatbot tutorial, we use a ``GreedySearchDecoder`` module to\n",
    "# facilitate the actual decoding process. This module has the trained\n",
    "# encoder and decoder models as attributes, and drives the process of\n",
    "# encoding an input sentence (a vector of word indexes), and iteratively\n",
    "# decoding an output response sequence one word (word index) at a time.\n",
    "#\n",
    "# Encoding the input sequence is straightforward: simply forward the\n",
    "# entire sequence tensor and its corresponding lengths vector to the\n",
    "# ``encoder``. It is important to note that this module only deals with\n",
    "# one input sequence at a time, **NOT** batches of sequences. Therefore,\n",
    "# when the constant **1** is used for declaring tensor sizes, this\n",
    "# corresponds to a batch size of 1. To decode a given decoder output, we\n",
    "# must iteratively run forward passes through our decoder model, which\n",
    "# outputs softmax scores corresponding to the probability of each word\n",
    "# being the correct next word in the decoded sequence. We initialize the\n",
    "# ``decoder_input`` to a tensor containing an *SOS_token*. After each pass\n",
    "# through the ``decoder``, we *greedily* append the word with the highest\n",
    "# softmax probability to the ``decoded_words`` list. We also use this word\n",
    "# as the ``decoder_input`` for the next iteration. The decoding process\n",
    "# terminates either if the ``decoded_words`` list has reached a length of\n",
    "# *MAX_LENGTH* or if the predicted word is the *EOS_token*.\n",
    "#\n",
    "# TorchScript Notes:\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# The ``forward`` method of this module involves iterating over the range\n",
    "# of :math:`[0, max\\_length)` when decoding an output sequence one word at\n",
    "# a time. Because of this, we should use **scripting** to convert this\n",
    "# module to TorchScript. Unlike with our encoder and decoder models,\n",
    "# which we can trace, we must make some necessary changes to the\n",
    "# ``GreedySearchDecoder`` module in order to initialize an object without\n",
    "# error. In other words, we must ensure that our module adheres to the\n",
    "# rules of the TorchScript mechanism, and does not utilize any language\n",
    "# features outside of the subset of Python that TorchScript includes.\n",
    "#\n",
    "# To get an idea of some manipulations that may be required, we will go\n",
    "# over the diffs between the ``GreedySearchDecoder`` implementation from\n",
    "# the chatbot tutorial and the implementation that we use in the cell\n",
    "# below. Note that the lines highlighted in red are lines removed from the\n",
    "# original implementation and the lines highlighted in green are new.\n",
    "#\n",
    "# .. figure:: /_static/img/chatbot/diff.png\n",
    "#    :align: center\n",
    "#    :alt: diff\n",
    "#\n",
    "# Changes:\n",
    "# ^^^^^^^^\n",
    "#\n",
    "# -  Added ``decoder_n_layers`` to the constructor arguments\n",
    "#\n",
    "#    -  This change stems from the fact that the encoder and decoder\n",
    "#       models that we pass to this module will be a child of\n",
    "#       ``TracedModule`` (not ``Module``). Therefore, we cannot access the\n",
    "#       decoder’s number of layers with ``decoder.n_layers``. Instead, we\n",
    "#       plan for this, and pass this value in during module construction.\n",
    "#\n",
    "#\n",
    "# -  Store away new attributes as constants\n",
    "#\n",
    "#    -  In the original implementation, we were free to use variables from\n",
    "#       the surrounding (global) scope in our ``GreedySearchDecoder``\\ ’s\n",
    "#       ``forward`` method. However, now that we are using scripting, we\n",
    "#       do not have this freedom, as the assumption with scripting is that\n",
    "#       we cannot necessarily hold on to Python objects, especially when\n",
    "#       exporting. An easy solution to this is to store these values from\n",
    "#       the global scope as attributes to the module in the constructor,\n",
    "#       and add them to a special list called ``__constants__`` so that\n",
    "#       they can be used as literal values when constructing the graph in\n",
    "#       the ``forward`` method. An example of this usage is on NEW line\n",
    "#       19, where instead of using the ``device`` and ``SOS_token`` global\n",
    "#       values, we use our constant attributes ``self._device`` and\n",
    "#       ``self._SOS_token``.\n",
    "#\n",
    "#\n",
    "# -  Enforce types of ``forward`` method arguments\n",
    "#\n",
    "#    -  By default, all parameters to a TorchScript function are assumed\n",
    "#       to be Tensor. If we need to pass an argument of a different type,\n",
    "#       we can use function type annotations as introduced in `PEP\n",
    "#       3107 <https://www.python.org/dev/peps/pep-3107/>`__. In addition,\n",
    "#       it is possible to declare arguments of different types using\n",
    "#       MyPy-style type annotations (see\n",
    "#       `doc <https://pytorch.org/docs/master/jit.html#types>`__).\n",
    "#\n",
    "#\n",
    "# -  Change initialization of ``decoder_input``\n",
    "#\n",
    "#    -  In the original implementation, we initialized our\n",
    "#       ``decoder_input`` tensor with ``torch.LongTensor([[SOS_token]])``.\n",
    "#       When scripting, we are not allowed to initialize tensors in a\n",
    "#       literal fashion like this. Instead, we can initialize our tensor\n",
    "#       with an explicit torch function such as ``torch.ones``. In this\n",
    "#       case, we can easily replicate the scalar ``decoder_input`` tensor\n",
    "#       by multiplying 1 by our SOS_token value stored in the constant\n",
    "#       ``self._SOS_token``.\n",
    "#\n",
    "\n",
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, decoder_n_layers):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self._device = device\n",
    "        self._SOS_token = SOS_token\n",
    "        self._decoder_n_layers = decoder_n_layers\n",
    "\n",
    "    __constants__ = ['_device', '_SOS_token', '_decoder_n_layers']\n",
    "\n",
    "    def forward(self, input_seq : torch.Tensor, input_length : torch.Tensor, max_length : int):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:self._decoder_n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=self._device, dtype=torch.long) * self._SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=self._device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=self._device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Evaluating an Input\n",
    "# ~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# Next, we define some functions for evaluating an input. The ``evaluate``\n",
    "# function takes a normalized string sentence, processes it to a tensor of\n",
    "# its corresponding word indexes (with batch size of 1), and passes this\n",
    "# tensor to a ``GreedySearchDecoder`` instance called ``searcher`` to\n",
    "# handle the encoding/decoding process. The searcher returns the output\n",
    "# word index vector and a scores tensor corresponding to the softmax\n",
    "# scores for each decoded word token. The final step is to convert each\n",
    "# word index back to its string representation using ``voc.index2word``.\n",
    "#\n",
    "# We also define two functions for evaluating an input sentence. The\n",
    "# ``evaluateInput`` function prompts a user for an input, and evaluates\n",
    "# it. It will continue to ask for another input until the user enters ‘q’\n",
    "# or ‘quit’.\n",
    "#\n",
    "# The ``evaluateExample`` function simply takes a string input sentence as\n",
    "# an argument, normalizes it, evaluates it, and prints the response.\n",
    "#\n",
    "\n",
    "def evaluate(searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # indexes -> words\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "# Evaluate inputs from user input (stdin)\n",
    "def evaluateInput(searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            # Evaluate sentence\n",
    "            output_words = evaluate(searcher, voc, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")\n",
    "\n",
    "# Normalize input sentence and call evaluate()\n",
    "def evaluateExample(sentence, searcher, voc):\n",
    "    print(\"> \" + sentence)\n",
    "    # Normalize sentence\n",
    "    input_sentence = normalizeString(sentence)\n",
    "    # Evaluate sentence\n",
    "    output_words = evaluate(searcher, voc, input_sentence)\n",
    "    output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "    print('Bot:', ' '.join(output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/save/cb_model/movie-corpus/2-2_500/4000_checkpoint.tar\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Load Pretrained Parameters\n",
    "# --------------------------\n",
    "#\n",
    "# Ok, its time to load our model!\n",
    "#\n",
    "# Use hosted model\n",
    "# ~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# To load the hosted model:\n",
    "#\n",
    "# 1) Download the model `here <https://download.pytorch.org/models/tutorials/4000_checkpoint.tar>`__.\n",
    "#\n",
    "# 2) Set the ``loadFilename`` variable to the path to the downloaded\n",
    "#    checkpoint file.\n",
    "#\n",
    "# 3) Leave the ``checkpoint = torch.load(loadFilename)`` line uncommented,\n",
    "#    as the hosted model was trained on CPU.\n",
    "#\n",
    "# Use your own model\n",
    "# ~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# To load your own pre-trained model:\n",
    "#\n",
    "# 1) Set the ``loadFilename`` variable to the path to the checkpoint file\n",
    "#    that you wish to load. Note that if you followed the convention for\n",
    "#    saving the model from the chatbot tutorial, this may involve changing\n",
    "#    the ``model_name``, ``encoder_n_layers``, ``decoder_n_layers``,\n",
    "#    ``hidden_size``, and ``checkpoint_iter`` (as these values are used in\n",
    "#    the model path).\n",
    "#\n",
    "# 2) If you trained the model on a CPU, make sure that you are opening the\n",
    "#    checkpoint with the ``checkpoint = torch.load(loadFilename)`` line.\n",
    "#    If you trained the model on a GPU and are running this tutorial on a\n",
    "#    CPU, uncomment the\n",
    "#    ``checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))``\n",
    "#    line.\n",
    "#\n",
    "# TorchScript Notes:\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# Notice that we initialize and load parameters into our encoder and\n",
    "# decoder models as usual. If you are using tracing mode(`torch.jit.trace`)\n",
    "# for some part of your models, you must call .to(device) to set the device\n",
    "# options of the models and .eval() to set the dropout layers to test mode\n",
    "# **before** tracing the models. `TracedModule` objects do not inherit the\n",
    "# ``to`` or ``eval`` methods. Since in this tutorial we are only using\n",
    "# scripting instead of tracing, we only need to do this before we do\n",
    "# evaluation (which is the same as we normally do in eager mode).\n",
    "#\n",
    "\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "corpus_name = \"movie-corpus\"\n",
    "\n",
    "# Configure models\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# If you're loading your own model\n",
    "# Set checkpoint to load from\n",
    "checkpoint_iter = 4000\n",
    "loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "                             '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "                             '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "print(loadFilename)\n",
    "\n",
    "# If you're loading the hosted model\n",
    "#loadFilename = 'data/4000_checkpoint.tar'\n",
    "\n",
    "# Load model\n",
    "# Force CPU device options (to match tensors in this tutorial)\n",
    "checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "encoder_sd = checkpoint['en']\n",
    "decoder_sd = checkpoint['de']\n",
    "encoder_optimizer_sd = checkpoint['en_opt']\n",
    "decoder_optimizer_sd = checkpoint['de_opt']\n",
    "embedding_sd = checkpoint['embedding']\n",
    "voc = Voc(corpus_name)\n",
    "voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "# Load trained model params\n",
    "encoder.load_state_dict(encoder_sd)\n",
    "decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/jit/_trace.py:154: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:485.)\n",
      "  if a.grad is not None:\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Convert Model to TorchScript\n",
    "# -----------------------------\n",
    "#\n",
    "# Encoder\n",
    "# ~~~~~~~\n",
    "#\n",
    "# As previously mentioned, to convert the encoder model to TorchScript,\n",
    "# we use **scripting**. The encoder model takes an input sequence and\n",
    "# a corresponding lengths tensor. Therefore, we create an example input\n",
    "# sequence tensor ``test_seq``, which is of appropriate size (MAX_LENGTH,\n",
    "# 1), contains numbers in the appropriate range\n",
    "# :math:`[0, voc.num\\_words)`, and is of the appropriate type (int64). We\n",
    "# also create a ``test_seq_length`` scalar which realistically contains\n",
    "# the value corresponding to how many words are in the ``test_seq``. The\n",
    "# next step is to use the ``torch.jit.trace`` function to trace the model.\n",
    "# Notice that the first argument we pass is the module that we want to\n",
    "# trace, and the second is a tuple of arguments to the module’s\n",
    "# ``forward`` method.\n",
    "#\n",
    "# Decoder\n",
    "# ~~~~~~~\n",
    "#\n",
    "# We perform the same process for tracing the decoder as we did for the\n",
    "# encoder. Notice that we call forward on a set of random inputs to the\n",
    "# traced_encoder to get the output that we need for the decoder. This is\n",
    "# not required, as we could also simply manufacture a tensor of the\n",
    "# correct shape, type, and value range. This method is possible because in\n",
    "# our case we do not have any constraints on the values of the tensors\n",
    "# because we do not have any operations that could fault on out-of-range\n",
    "# inputs.\n",
    "#\n",
    "# GreedySearchDecoder\n",
    "# ~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# Recall that we scripted our searcher module due to the presence of\n",
    "# data-dependent control flow. In the case of scripting, we do necessary\n",
    "# language changes to make sure the implementation complies with\n",
    "# TorchScript. We initialize the scripted searcher the same way that we\n",
    "# would initialize an un-scripted variant.\n",
    "#\n",
    "\n",
    "### Compile the whole greedy search model to TorchScript model\n",
    "# Create artificial inputs\n",
    "test_seq = torch.LongTensor(MAX_LENGTH, 1).random_(0, voc.num_words).to(device)\n",
    "test_seq_length = torch.LongTensor([test_seq.size()[0]]).to(device)\n",
    "# Trace the model\n",
    "traced_encoder = torch.jit.trace(encoder, (test_seq, test_seq_length))\n",
    "\n",
    "### Convert decoder model\n",
    "# Create and generate artificial inputs\n",
    "test_encoder_outputs, test_encoder_hidden = traced_encoder(test_seq, test_seq_length)\n",
    "test_decoder_hidden = test_encoder_hidden[:decoder.n_layers]\n",
    "test_decoder_input = torch.LongTensor(1, 1).random_(0, voc.num_words)\n",
    "# Trace the model\n",
    "traced_decoder = torch.jit.trace(decoder, (test_decoder_input, test_decoder_hidden, test_encoder_outputs))\n",
    "\n",
    "### Initialize searcher module by wrapping ``torch.jit.script`` call\n",
    "scripted_searcher = torch.jit.script(GreedySearchDecoder(traced_encoder, traced_decoder, decoder.n_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scripted_searcher graph:\n",
      " graph(%self : __torch__.GreedySearchDecoder,\n",
      "      %input_seq.1 : Tensor,\n",
      "      %input_length.1 : Tensor,\n",
      "      %max_length.1 : int):\n",
      "  %53 : bool = prim::Constant[value=0]()\n",
      "  %42 : bool = prim::Constant[value=1]() # /var/folders/rk/rb0btt_x2mxgsj16ky_t07tr0000gn/T/ipykernel_1542/3627691502.py:127:8\n",
      "  %18 : int = prim::Constant[value=4]() # /var/folders/rk/rb0btt_x2mxgsj16ky_t07tr0000gn/T/ipykernel_1542/3627691502.py:122:68\n",
      "  %17 : Device = prim::Constant[value=\"cpu\"]() # /var/folders/rk/rb0btt_x2mxgsj16ky_t07tr0000gn/T/ipykernel_1542/3627691502.py:122:48\n",
      "  %14 : NoneType = prim::Constant()\n",
      "  %12 : int = prim::Constant[value=2]() # /var/folders/rk/rb0btt_x2mxgsj16ky_t07tr0000gn/T/ipykernel_1542/3627691502.py:120:41\n",
      "  %16 : int = prim::Constant[value=1]() # /var/folders/rk/rb0btt_x2mxgsj16ky_t07tr0000gn/T/ipykernel_1542/3627691502.py:122:35\n",
      "  %26 : int = prim::Constant[value=0]() # /var/folders/rk/rb0btt_x2mxgsj16ky_t07tr0000gn/T/ipykernel_1542/3627691502.py:124:34\n",
      "  %encoder : __torch__.model.EncoderRNN = prim::GetAttr[name=\"encoder\"](%self)\n",
      "  %7 : (Tensor, Tensor) = prim::CallMethod[name=\"forward\"](%encoder, %input_seq.1, %input_length.1) # /var/folders/rk/rb0btt_x2mxgsj16ky_t07tr0000gn/T/ipykernel_1542/3627691502.py:118:42\n",
      "  %encoder_outputs.1 : Tensor, %encoder_hidden.1 : Tensor = prim::TupleUnpack(%7)\n",
      "  %decoder_hidden.1 : Tensor = aten::slice(%encoder_hidden.1, %26, %14, %12, %16) # /var/folders/rk/rb0btt_x2mxgsj16ky_t07tr0000gn/T/ipykernel_1542/3627691502.py:120:25\n",
      "  %20 : int[] = prim::ListConstruct(%16, %16)\n",
      "  %23 : Tensor = aten::ones(%20, %18, %14, %17, %14) # /var/folders/rk/rb0btt_x2mxgsj16ky_t07tr0000gn/T/ipykernel_1542/3627691502.py:122:24\n",
      "  %decoder_input.1 : Tensor = aten::mul(%23, %16) # /var/folders/rk/rb0btt_x2mxgsj16ky_t07tr0000gn/T/ipykernel_1542/3627691502.py:122:24\n",
      "  %27 : int[] = prim::ListConstruct(%26)\n",
      "  %all_tokens.1 : Tensor = aten::zeros(%27, %18, %14, %17, %14) # /var/folders/rk/rb0btt_x2mxgsj16ky_t07tr0000gn/T/ipykernel_1542/3627691502.py:124:21\n",
      "  %33 : int[] = prim::ListConstruct(%26)\n",
      "  %all_scores.1 : Tensor = aten::zeros(%33, %14, %14, %17, %14) # /var/folders/rk/rb0btt_x2mxgsj16ky_t07tr0000gn/T/ipykernel_1542/3627691502.py:125:21\n",
      "  %all_tokens : Tensor, %all_scores : Tensor, %decoder_hidden : Tensor, %decoder_input : Tensor = prim::Loop(%max_length.1, %42, %all_tokens.1, %all_scores.1, %decoder_hidden.1, %decoder_input.1) # /var/folders/rk/rb0btt_x2mxgsj16ky_t07tr0000gn/T/ipykernel_1542/3627691502.py:127:8\n",
      "    block0(%43 : int, %all_tokens.11 : Tensor, %all_scores.11 : Tensor, %decoder_hidden.9 : Tensor, %decoder_input.17 : Tensor):\n",
      "      %decoder : __torch__.model.LuongAttnDecoderRNN = prim::GetAttr[name=\"decoder\"](%self)\n",
      "      %48 : (Tensor, Tensor) = prim::CallMethod[name=\"forward\"](%decoder, %decoder_input.17, %decoder_hidden.9, %encoder_outputs.1) # /var/folders/rk/rb0btt_x2mxgsj16ky_t07tr0000gn/T/ipykernel_1542/3627691502.py:129:45\n",
      "      %decoder_output.1 : Tensor, %decoder_hidden.5 : Tensor = prim::TupleUnpack(%48)\n",
      "      %decoder_scores.1 : Tensor, %decoder_input.5 : Tensor = aten::max(%decoder_output.1, %16, %53) # /var/folders/rk/rb0btt_x2mxgsj16ky_t07tr0000gn/T/ipykernel_1542/3627691502.py:131:44\n",
      "      %61 : Tensor[] = prim::ListConstruct(%all_tokens.11, %decoder_input.5)\n",
      "      %all_tokens.5 : Tensor = aten::cat(%61, %26) # /var/folders/rk/rb0btt_x2mxgsj16ky_t07tr0000gn/T/ipykernel_1542/3627691502.py:133:25\n",
      "      %67 : Tensor[] = prim::ListConstruct(%all_scores.11, %decoder_scores.1)\n",
      "      %all_scores.5 : Tensor = aten::cat(%67, %26) # /var/folders/rk/rb0btt_x2mxgsj16ky_t07tr0000gn/T/ipykernel_1542/3627691502.py:134:25\n",
      "      %decoder_input.13 : Tensor = aten::unsqueeze(%decoder_input.5, %26) # /var/folders/rk/rb0btt_x2mxgsj16ky_t07tr0000gn/T/ipykernel_1542/3627691502.py:136:28\n",
      "      -> (%42, %all_tokens.5, %all_scores.5, %decoder_hidden.5, %decoder_input.13)\n",
      "  %75 : (Tensor, Tensor) = prim::TupleConstruct(%all_tokens, %all_scores)\n",
      "  return (%75)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Print Graphs\n",
    "# ------------\n",
    "#\n",
    "# Now that our models are in TorchScript form, we can print the graphs of\n",
    "# each to ensure that we captured the computational graph appropriately.\n",
    "# Since TorchScript allow us to recursively compile the whole model\n",
    "# hierarchy and inline the ``encoder`` and ``decoder`` graph into a single\n",
    "# graph, we just need to print the `scripted_searcher` graph\n",
    "\n",
    "print('scripted_searcher graph:\\n', scripted_searcher.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> hello\n",
      "Bot: hello\n",
      "> what's up?\n",
      "Bot: i am not going to get out of here\n",
      "> who are you?\n",
      "Bot: i am not\n",
      "> where am I?\n",
      "Bot: i am not sure\n",
      "> where are you from?\n",
      "Bot: helsinki\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Run Evaluation\n",
    "# --------------\n",
    "#\n",
    "# Finally, we will run evaluation of the chatbot model using the TorchScript\n",
    "# models. If converted correctly, the models will behave exactly as they\n",
    "# would in their eager-mode representation.\n",
    "#\n",
    "# By default, we evaluate a few common query sentences. If you want to\n",
    "# chat with the bot yourself, uncomment the ``evaluateInput`` line and\n",
    "# give it a spin.\n",
    "#\n",
    "\n",
    "\n",
    "# Use appropriate device\n",
    "scripted_searcher.to(device)\n",
    "# Set dropout layers to eval mode\n",
    "scripted_searcher.eval()\n",
    "\n",
    "# Evaluate examples\n",
    "sentences = [\"hello\", \"what's up?\", \"who are you?\", \"where am I?\", \"where are you from?\"]\n",
    "for s in sentences:\n",
    "    evaluateExample(s, scripted_searcher, voc)\n",
    "\n",
    "# Evaluate your input\n",
    "#evaluateInput(traced_encoder, traced_decoder, scripted_searcher, voc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# Save Model\n",
    "# ----------\n",
    "#\n",
    "# Now that we have successfully converted our model to TorchScript, we\n",
    "# will serialize it for use in a non-Python deployment environment. To do\n",
    "# this, we can simply save our ``scripted_searcher`` module, as this is\n",
    "# the user-facing interface for running inference against the chatbot\n",
    "# model. When saving a Script module, use script_module.save(PATH) instead\n",
    "# of torch.save(model, PATH).\n",
    "#\n",
    "\n",
    "scripted_searcher.save(\"scripted_chatbot.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
